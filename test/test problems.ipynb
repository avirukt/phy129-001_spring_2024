{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test problems"
   ],
   "id": "0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Maximum Entropy Probability Assignment"
   ],
   "id": "1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- understand how probabilities are assigned when relevant information is unknown\n",
    "- apply the principle and mathematical framework to a simple system\n",
    "- extend the concept to other definitions of entropy within the simple system using numerical tools\n"
   ],
   "id": "2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "\n",
    "\n",
    "Recall that the _Principle of Maximum Entropy_ provides a fundamental rationale for assigning prior probability distributions in statistical mechanics.  By using variational principle involving the constrained maximum of the Shannon entropy, we ensure that the distribution agrees with everything relevant we do know, but do not pretend to information we do not know.\n",
    "\n",
    "In particular,  suppose we require a categorical probability distribution $p_1, \\dotsc, p_n$ over a total of $n$ discrete possibilities $x_1, \\dotsc, x_n$, considered mutually exclusive and exhaustive.  We therefore know that $p_j \\geq 0$ for all $j = 1, \\dotsc, n$, while $\\sum\\limits_{j=1}^{n} p_j = 1$.  In addition, we somehow know (or assume) the values of one or more expectation values,\n",
    "\n",
    "\\begin{align}\n",
    "\\langle f \\rangle = \\sum\\limits_{j=1}^{n} p_j \\, f(x_j)   &= \\bar{f} ,\\\\\n",
    "\\langle g \\rangle = \\sum\\limits_{j=1}^{n} p_j \\, g(x_j)   &= \\bar{g} ,\\\\\n",
    "&\\text{etc.}\n",
    "\\end{align}\n",
    "\n",
    "Then the distribution maximizing the Shannon entropy\n",
    "$$\n",
    "S = - \\sum\\limits_{j=1}^n p_j \\log p_j\n",
    "$$\n",
    "subject to the normalization and average-value constraints is of the \"generalized canonical\" form\n",
    "$$\n",
    "p_j = \\frac{ e^{- \\lambda f(x_j) - \\mu g(x_j) + \\dotsb} }{ Z(\\lambda, \\mu, \\dotsc) } ,   \\;\\; j = 1, \\dotsc, n\n",
    "$$\n",
    "where the _partition function_\n",
    "$$\n",
    "Z(\\lambda, \\mu, \\dotsc) = \\sum\\limits_{j= 1}^{n} e^{- \\lambda f(x_j) - \\mu g(x_j) + \\dotsb }\n",
    "$$\n",
    "ensures proper normalization, while the Lagrange multipliers $\\lambda, \\mu, \\dotsc$ are chosen so as to satisfy the constraints on the expectation values:\n",
    "\n",
    "\\begin{align}\n",
    "-\\tfrac{\\partial}{\\partial \\lambda} \\ln Z &= \\langle f \\rangle = \\bar{f} ,\\\\\n",
    "-\\tfrac{\\partial}{\\partial \\mu} \\ln Z &= \\langle g\\rangle = \\bar{g} , \\\\\n",
    "&\\text{etc.}.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "NOTE:  many sources suggest determining the Lagrange multipliers by finding the simultaneous solutions of these equations.  But generally, it is not a good idea to optimize a function just by looking for the zeros of the gradient\u2014root-finding algorithms tend to be less efficient and less robust than function-minimization algorithms, because if nothing else works, the latter can always make progress by moving downhill.  It turns out that for fixed values of $\\bar{f}, \\bar{g}, \\dotsc$, the auxiliary function $\\Psi(\\lambda, \\mu, \\dotsc) =  \\ln Z(\\lambda, \\mu, \\dotsc) + \\lambda \\bar{f} + \\mu \\bar{g} + \\dotsb$ will be _minimized_ (not maximized, surprisingly...) at the desired values of $\\lambda, \\mu, \\dotsc$. But then the maximized value of the entropy is given by \n",
    "$$\n",
    "S(\\bar{f}, \\bar{g}, \\dotsc) = \\ln Z(\\lambda, \\mu, \\dotsc) + \\lambda \\bar{f} + \\mu \\bar{g} + \\dotsb,\n",
    "$$\n",
    "with the Lagrange multipliers so chosen.\n"
   ],
   "id": "3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. "
   ],
   "id": "4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "As an example, consider throws of one, possibly imbalanced, but otherwise standard die,\n",
    "with the $j$th side marked with $x_j = j$ pips for $j = 1, \\dotsc, 6$.\n",
    "\n",
    "If the die is \"fair,\" then the expected value of the next throw would be $\\bar{x} = \\tfrac{1}{6}( 1 + \\dotsb + 6) = 3.5$  Instead, suppose that $\\bar{x} = 4.5$.  Obviously, the die (and/or the throwing mechanism) is weighted in favor of higher numbers, but by how much?\n",
    "\n",
    "If this is all we know, the best we can do is use the maximum entropy distribution agreeing with this average.\n",
    "\n",
    "Find the normalized, maximum-entropy distribution $p_1, \\dotsc, p_6$ corresponding to the average $\\bar{x} = 4.5$."
   ],
   "id": "5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. "
   ],
   "id": "7"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "\n",
    "\n",
    "What is the entropy of this distribution?  How does the entropy compare to the case of the fair die?\n"
   ],
   "id": "8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. "
   ],
   "id": "10"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "What is the standard deviation $\\sigma_{x}$ of the number of pips, given this maximum-entropy distribution? Compare to a fair die."
   ],
   "id": "11"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. "
   ],
   "id": "13"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Other variational principles have been suggested (albeit with questionalbe justification) for assigning probability distributions.\n",
    "\n",
    "Find the distribution $p_1, \\dotsc, p_6$ with maximum variance amongst all distributions with $\\bar{x} = 4.5$.\n",
    "(Make sure the probabilities are also nonnegative and normalized)."
   ],
   "id": "14"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. "
   ],
   "id": "16"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "What is the entropy of this distribution? In, say, $10\\,000$ throws of the die, how many more ways are there to arrive at frequencies near the maximum entropy probabilities, compared to frequencies near the maximum-variance probabilities (as a factor)?\n",
    "\n",
    "Of course, maximizing variance cannot possible work as a general variational principle, if variance itself might be a constraint...."
   ],
   "id": "17"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1f. "
   ],
   "id": "19"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "\n",
    "\n",
    "Find the distribution $p_1, \\dotsc, p_6$ with maximum value of $-\\log \\Bigl[ \\sum\\limits_{j=1}^{6} p_j^2 \\Bigr]$, amongst  all (non-negative and normalized) distributions with $\\bar{x} = 4.5$.  This quantity is known as the _collision entropy_, or as the order-$2$ R\u00e9nyi entropy.  Note that, unlike the case of Shannon entropy, non-negativity is not automatically guaranteed, but must be imposed as an explicit constraint."
   ],
   "id": "20"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1g. "
   ],
   "id": "22"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "What is the Shannon entropy of the resulting distribution? In, say, $10\\,000$ throws of the die, how many more ways are there to arrive at frequencies near the maximum-entropy probabilities, compared to frequencies near the maximum-collision-entropy probabilities?\n"
   ],
   "id": "23"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1h. "
   ],
   "id": "25"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Finally find the distribution $p_1, \\dotsc, p_6$ with maximum value of the so-called _min-entropy_ $\\min\\limits_j \\log \\tfrac{1}{p_j}$, amongst  all (non-negative and normalized) distributions with $\\bar{x} = 4.5$. "
   ],
   "id": "26"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1i. "
   ],
   "id": "28"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    " \n",
    "\n",
    "What is the Shannon entropy of the resulting distribution? In, say, $10\\,000$ throws of the die, how many more ways are there to arrive at frequencies near the maximum-entropy probabilities, compared to frequencies near the maximum-min-entropy probabilities?"
   ],
   "id": "29"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1j. "
   ],
   "id": "31"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Make a plot of the three entropies as a function of $\\bar{x}$."
   ],
   "id": "32"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Write your answer here"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "33"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}